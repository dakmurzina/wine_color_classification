{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression to classify wine color (Red, White, Rose, unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training data from a csv file with pandas\n",
    "import pandas\n",
    "\n",
    "train_set = pandas.read_csv('./train.csv', sep='\\t', encoding='utf-8')\n",
    "train_set\n",
    "\n",
    "# Let's extract only these two columns from the data \n",
    "train_reviews = train_set['Review'].to_list()\n",
    "train_colors = train_set['Color'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Creating zero vectors with numpy\n",
    "import numpy\n",
    "\n",
    "# assigning the list of terms to a variable\n",
    "terms = [\"tannin\", \"cherry\", \"oak\", \"fresh\", \"vanilla\", \"rich\", \"blackberry\", \"very\", \"dry\", \"spice\"]\n",
    "\n",
    "# Creating zero vectors of length 10 for each review in our training set\n",
    "# numpy.zeros(X, Y) \n",
    "# X = no. of rows, Y = no. of columns\n",
    "train_features = numpy.zeros((len(train_reviews), len(terms)))\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spacy to look at the lemmas of words (tokens) in the review and compare them with terms from the terms list\n",
    "import spacy\n",
    "\n",
    "# Using small language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Processing a text\n",
    "train_doc_reviews = nlp.pipe(train_reviews)\n",
    "\n",
    "\n",
    "# looping over each review, label and feature vector at the same time (zip)\n",
    "for review, f, c in zip(train_doc_reviews, train_features, train_colors):\n",
    "    tokens_list = [token.lemma_ for token in review]\n",
    "    #print(tokens_list)\n",
    "    for term in terms:\n",
    "        if term in tokens_list:\n",
    "            term_id = terms.index(term)\n",
    "            f[term_id] = 1 # replacing the vector value with 1 at the index where the term is in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Red' 'Rose' 'White' 'unk']\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Importing LogisticRegression model from scikit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Making an instance of the Model from LogisticRegression class\n",
    "# all parameters not specified are set to their defaults\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Training the model on the data, storing the information learned from the data\n",
    "# Model is learning the relationship between digits (x_train) and labels (y_train)\n",
    "lr.fit(train_features, train_colors)\n",
    "\n",
    "# Let's see what are the possible labels to predict (and in which order they are stored)\n",
    "print(lr.classes_)\n",
    "\n",
    "# We can get additional information about all the parameters used with LogReg model\n",
    "print(lr.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openning test dataset with pandas.read_csv\n",
    "import pandas\n",
    "test_set = pandas.read_csv('./test.csv', sep='\\t', encoding='utf-8')\n",
    "test_set\n",
    "\n",
    "\n",
    "# Extracting only the relevant columns, and puting them in lists\n",
    "test_reviews = test_set['Review'].to_list()\n",
    "test_colors = test_set['Color'].to_list()\n",
    "\n",
    "# Creating zero vectors (of length 10) for each review (of len(test_reviews))\n",
    "test_features = numpy.zeros((len(test_reviews), len(terms)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spacy to process the text and see if the lemmas of the words in reviews match the terms from the term list\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Processing a text\n",
    "test_doc_reviews = nlp.pipe(test_reviews)\n",
    "\n",
    "# Updating the feature vectors by checkin if the terms exist per review\n",
    "for review, f_vector in zip(test_doc_reviews, test_features):\n",
    "    tokens_list = [token.lemma_ for token in review]\n",
    "    for term in terms:\n",
    "        if term in tokens_list:\n",
    "            term_id = terms.index(term)\n",
    "            f_vector[term_id]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a punction predict which will take a review index as input and prints information about correct label and predicted label\n",
    "def predict(i):\n",
    "    # printing the review of the index\n",
    "    print(test_reviews[i])\n",
    "    # printing the features of the index\n",
    "    print(test_features[i])\n",
    "    # printing all terms\n",
    "    print(terms)\n",
    "    # printing the correct label of the index\n",
    "    print(test_colors[i])\n",
    "\n",
    "    print()\n",
    "    print(\"Prediction:\")\n",
    "    # printing the prediction for the features of this index\n",
    "    print(lr.predict([test_features[i]]))\n",
    "    # printing the probabilities for each label predictions\n",
    "    print(lr.predict_proba([test_features[i]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leather, spice, tobacco and tea emerge from the nose of this Sicilian blend of Nero d’Avola, Syrah, Merlot, Cabernet and Petit Verdot. You’ll get aromas of clove, allspice and vanilla behind vibrant blueberry and raspberry.\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 1.]\n",
      "['tannin', 'cherry', 'oak', 'fresh', 'vanilla', 'rich', 'blackberry', 'very', 'dry', 'spice']\n",
      "Red\n",
      "\n",
      "Prediction:\n",
      "['Red']\n",
      "[[0.44869982 0.02516404 0.44414481 0.08199132]]\n",
      "\n",
      "I haven’t been a fan of Santa Ynez Cabs for the simple reason that they’re so seldom ripe. You get this green, herb and mint streak that’s not flattering to Cab’s tannins. This wine is in that vein. \n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['tannin', 'cherry', 'oak', 'fresh', 'vanilla', 'rich', 'blackberry', 'very', 'dry', 'spice']\n",
      "Red\n",
      "\n",
      "Prediction:\n",
      "['Red']\n",
      "[[0.94900612 0.00379494 0.01881589 0.02838305]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calling the function predict to predict the label for the reviews at indexes 0 and 10\n",
    "\n",
    "predict(0)\n",
    "predict(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Red' 'Rose' 'White' 'unk']\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# saving the model to a binary file\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Saving to file in the current working directory\n",
    "pkl_filename = \"lr_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(lr, file)\n",
    "\n",
    "# Loading from file\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Let's see what are the possible labels to predict (and in which order they are stored)\n",
    "print(pickle_model.classes_)\n",
    "\n",
    "# We can get additional information about all the parameters used with LogReg model\n",
    "print(pickle_model.get_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
