{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QGZW4Y1X8zi0"
      },
      "source": [
        "# Machine Learning - Evaluating Classifiers\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HOquqlhu8zi3"
      },
      "source": [
        "\n",
        "## Evaluating classifiers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "r-Zt80W88zi4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Red' 'Rose' 'White' 'unk']\n",
            "Red\n",
            "tannin 2.383661939736888\n",
            "cherry 1.9274105719068597\n",
            "oak 1.3050397713743673\n",
            "fresh -0.7187393751756458\n",
            "vanilla -0.35856506673104627\n",
            "rich -0.2262790743078582\n",
            "blackberry 2.4503412762346746\n",
            "very 0.1414245846276523\n",
            "dry -0.24413649876702623\n",
            "spice 0.6519265614229908\n",
            "INTERCEPT 0.8543590705888237\n",
            "\n",
            "Rose\n",
            "tannin -0.6266939262327331\n",
            "cherry 1.3921244363441443\n",
            "oak -1.4622530150523196\n",
            "fresh 0.49686452802491615\n",
            "vanilla 0.305723523004968\n",
            "rich -0.4962346320419678\n",
            "blackberry 0.3396785232108212\n",
            "very -0.41421327477594533\n",
            "dry 0.14391027564668946\n",
            "spice -0.3819085563059017\n",
            "INTERCEPT -1.6570325008203965\n",
            "\n",
            "White\n",
            "tannin -1.8945664666243693\n",
            "cherry -2.858257960755354\n",
            "oak 1.0538210926332086\n",
            "fresh 0.10189358829165852\n",
            "vanilla 0.059132997176185774\n",
            "rich 0.2344474242024624\n",
            "blackberry -2.3499664049992304\n",
            "very -0.007593425443304999\n",
            "dry -0.13199375581460723\n",
            "spice -0.13348973242972784\n",
            "INTERCEPT 1.211873847292693\n",
            "\n",
            "unk\n",
            "tannin 0.13759845312021848\n",
            "cherry -0.4612770474956476\n",
            "oak -0.8966078489552564\n",
            "fresh 0.11998125885907\n",
            "vanilla -0.006291453450112956\n",
            "rich 0.48806628214737013\n",
            "blackberry -0.44005339444625774\n",
            "very 0.2803821155915986\n",
            "dry 0.23221997893494198\n",
            "spice -0.13652827268736253\n",
            "INTERCEPT -0.40920041706107785\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# importing the classification model\n",
        "import pickle\n",
        "\n",
        "# list of terms we used as features\n",
        "terms = [\"tannin\", \"cherry\", \"oak\", \"fresh\", \"vanilla\", \"rich\", \"blackberry\", \"very\", \"dry\", \"spice\"]\n",
        "\n",
        "\n",
        "# Loading the model with 10 features (terms)\n",
        "with open('lr_model.pkl', 'rb') as file:\n",
        "    lr_10 = pickle.load(file)\n",
        "    \n",
        "# Looking at the possible labels to predict (and in which order they are stored)\n",
        "print(lr_10.classes_)\n",
        "\n",
        "# Print each label and corresponding coefficients and intercept\n",
        "for label, coefs, intercept in zip(lr_10.classes_, lr_10.coef_, lr_10.intercept_):\n",
        "    print(label)\n",
        "    for term, coef in zip(terms, coefs):\n",
        "        print(term, coef)\n",
        "    print(f\"INTERCEPT {intercept}\")\n",
        "    print()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "boFFgCIG8zi5"
      },
      "source": [
        "Features extraction for the test set to get predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ehnU7W9A8zi5",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leather, spice, tobacco and tea emerge from the nose of this Sicilian blend of Nero d’Avola, Syrah, Merlot, Cabernet and Petit Verdot. You’ll get aromas of clove, allspice and vanilla behind vibrant blueberry and raspberry.\n",
            "Red\n",
            "\n",
            "So pale that it’s almost colorless, the Blangé—made from Arneis grapes in Piedmont—has the oak-meets-citrus nose you’d expect of a Chardonnay. The lemon, grapefruit and pear flavors, coupled with a superspritzy, Asti-like mouthfeel, make this a good Sunday brunch eye-opener. Its slightly high acidity could cut through any French toast or waffle that you throw its way. Finishes medium-long, with herbal notes. \n",
            "White\n",
            "\n",
            "Comes across on the earthy, herbal side, although there are some pretty raspberry notes on the palate. A little heavy, and lacks perhaps some delicacy. A sweet cola-like finish completes the picture.\n",
            "Red\n",
            "\n",
            "What a wonderful wine to pair with spaghetti alle vongole or calamari fritti. Made from Grechetto grapes and aged only in stainless steel, this is fresh, youthful, tangy and crisp, with aromas of citrus, kiwi, honeydew and peach.\n",
            "White\n",
            "\n",
            "What’s puzzling about this wine is why, given the pedigree of the vines, it was so quickly made and rushed to market. It is not a distinctive wine, but a rustic one, dry and berryish. Cline could have done better.\n",
            "Red\n",
            "\n",
            "(1000, 10)\n"
          ]
        }
      ],
      "source": [
        "import pandas\n",
        "import numpy\n",
        "\n",
        "# Reading test set and making the feature vectors\n",
        "test_set = pandas.read_csv('./test.csv', sep='\\t', encoding='utf-8')\n",
        "test_set\n",
        "\n",
        "# Extracting only the relevant columns, and puting them in lists\n",
        "# These colums have the titles 'Review' and 'Color'\n",
        "test_reviews = test_set['Review'].to_list()\n",
        "test_colors = test_set['Color'].to_list()\n",
        "\n",
        "# Printing the first five item in each list to make sure it looks ok\n",
        "for rev, col in zip(test_reviews[:5], test_colors[:5]):\n",
        "  print(rev)\n",
        "  print(col)\n",
        "  print()\n",
        "\n",
        "# Creating zero vectors (of length 10) for each review (of len(test_reviews))\n",
        "test_features = numpy.zeros((len(test_reviews), 10))\n",
        "print(test_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7GZdq4u28zi6"
      },
      "outputs": [],
      "source": [
        "# Generating features\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Processing a text\n",
        "test_doc_reviews = nlp.pipe(test_reviews)\n",
        "\n",
        "# Updating the feature vectors by checkin if the terms exist per review\n",
        "for review, f in zip(test_doc_reviews, test_features):\n",
        "    tokens_list = [token.lemma_ for token in review]\n",
        "    #print(tokens_list)\n",
        "    for term in terms:\n",
        "        if term in tokens_list:\n",
        "            term_id = terms.index(term)\n",
        "            f[term_id] = 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "oOYTbnBH8zi7"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DItrqUCi9MZp"
      },
      "outputs": [],
      "source": [
        "# Pretiction using the features as input to the model\n",
        "test_predictions = lr_10.predict(test_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_y2nLtN9fA3"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "Correct predictions divided by the total number of predictions (ratio of correct predictions to all predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IPt7ekBV9S5p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sklearn accuracy: 0.723\n",
            "Total reviews: 1000\n",
            "Total correct predictions: 723\n",
            "Correct ratio (accuracy): 0.723\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# We can calculate the fraction of correctly predicted labels in the test set\n",
        "# this is the accuracy score\n",
        "acc = accuracy_score(test_colors, test_predictions)\n",
        "corr_count = accuracy_score(test_colors, test_predictions, normalize = False)\n",
        "total_count=len(test_reviews)\n",
        "\n",
        "print(f\"Sklearn accuracy: {acc}\")\n",
        "print(f\"Total reviews: {total_count}\")\n",
        "print(f\"Total correct predictions: {corr_count}\")\n",
        "\n",
        "corr_ratio = corr_count /total_count\n",
        "print(f\"Correct ratio (accuracy): {corr_ratio}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ibtVRYm8zi-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Let's load the training data from a csv file\n",
        "test_set = pandas.read_csv('./test.csv', sep='\\t', encoding='utf-8')\n",
        "test_colors = test_set['Color'].to_list()\n",
        "\n",
        "\n",
        "# Get a dictionary of unique items with their counts\n",
        "print(Counter(test_colors))\n",
        "\n",
        "\n",
        "# Get the confusion matrix\n",
        "labels=[\"Red\", \"White\", \"Rose\", \"unk\"]\n",
        "???\n",
        "# The order of the labels is not important, but it is important to provide all the labels.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F-score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IPdcUu5-Kt44"
      },
      "source": [
        "#### Confusion matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "paB_yTie8zi_",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Red</th>\n",
              "      <th>White</th>\n",
              "      <th>Rose</th>\n",
              "      <th>unk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Red</th>\n",
              "      <td>432</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>White</th>\n",
              "      <td>19</td>\n",
              "      <td>291</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Rose</th>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unk</th>\n",
              "      <td>10</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Red  White  Rose  unk\n",
              "Red    432    168     0    0\n",
              "White   19    291     0    0\n",
              "Rose     4     15     0    0\n",
              "unk     10     61     0    0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "my_labels=[\"Red\", \"White\", \"Rose\", \"unk\"]\n",
        "cm1 = confusion_matrix(test_colors, test_predictions, labels = my_labels)\n",
        "# The order of the labels is not important, but it is important to provide all the labels.\n",
        "pd.DataFrame(cm1, index=my_labels, columns=my_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Red       0.93      0.72      0.81       600\n",
            "        Rose       0.00      0.00      0.00        19\n",
            "       White       0.54      0.94      0.69       310\n",
            "         unk       0.00      0.00      0.00        71\n",
            "\n",
            "    accuracy                           0.72      1000\n",
            "   macro avg       0.37      0.41      0.38      1000\n",
            "weighted avg       0.73      0.72      0.70      1000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Dinara Akmurzina\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\Dinara Akmurzina\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\Dinara Akmurzina\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(test_colors, test_predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3-new",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
